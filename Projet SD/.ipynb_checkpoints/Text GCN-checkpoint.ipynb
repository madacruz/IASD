{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1a35a1",
   "metadata": {},
   "source": [
    "# Expérimentations - GCN for text classification - Dataset R8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89a3f0",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons tenter de recoder l'algorithme décrit dans le papier \"GCN for text classification\", et d'effectuer des expériences autour de celui-ci.\n",
    "\n",
    "Notre objectif sera de classifier correctement des textes, en n'ayant qu'une petite base d'entrainement.\n",
    "\n",
    "A partir d'un corpus de textes, dont seulement un petit pourcentage est labelisé, nous soihaitons attribuer des classes à tous les documents. Le principe sera de répandre les classes des textes labelisés grâce à un GCN. Pour cela, nous devrons donc représenter nos données sous forme d'un graphe.\n",
    "\n",
    "Nous allons donc construire un graphe hétérogène dont les sommets seront des mots (apparaisant dans les documents) et des documents. Les arêtes représenteront des liens entre des mots, ou bien entre mots et documents, mais il n'y a pas d'arêtes entre documents. \n",
    "\n",
    "Le graphe contiendra tous les documents du corpus, c'est à dire autant le test set que le train set. Néanmoins, seul un petit pourcentage de ces documents disposerons de labels.\n",
    "\n",
    "Une fois le graphe construit nous pourrons lui appliquer un Graph Convolutional Network, dans le but de classifier les documents non labelisés.\n",
    "\n",
    "## EXPLIQUER ICI FONCTIONNEMENT DU GCN\n",
    "\n",
    "Nous pourrons donc mesurer les performances de notre modèle en regardant le taux de documents bien classifiés (parmi les documents qui ne disposaient pas de labels avant le GCN bien sur).\n",
    "\n",
    "Nous pourrons enfin tenter diverses expérimentations, notamment :\n",
    "\n",
    "    - Essayer plusieurs pourcentages de données labelisées\n",
    "    - embedding à la place  de pmi (developper)\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a845b88",
   "metadata": {},
   "source": [
    "### Table des matières <a name=\"up\"></a>\n",
    "\n",
    "* [1 Import des textes](#import)\n",
    "* [2 Préparation des données](#init)\n",
    "* [3 Calcul du TFIDF et du PMI](#dico)\n",
    "* [4 Construction du graphe](#tfidf)\n",
    "* [5 Text Rank](#tr)\n",
    "* [6 affichage docx](#docx)\n",
    "* [7 keywords](#keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0954b6b",
   "metadata": {},
   "source": [
    "Nous commençons par importer les librairies dont nous aurons besoin.\n",
    "\n",
    "Parmi celles-ci, plusieurs librairies de NLP, qui nous servirons notamment pour le pré-processing des données, et la tokenization (c'est à dire pour séparer les documents en mots). Nous aurons également besoin de la librairie networkx qui permet de gérer les graphes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f44db7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#math\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "#NLP\n",
    "import gensim\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import matutils\n",
    "from convectors.layers import (Contract, Lemmatize, Phrase, TfIdf,Tokenize)\n",
    "\n",
    "#Affichage du temps et accès aux données\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#Structure de données\n",
    "from scipy.sparse import csc_matrix, lil_matrix, csr_matrix\n",
    "\n",
    "#Graphes\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from textgraph.graph import TextGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02dfed",
   "metadata": {},
   "source": [
    "# 1 Import des textes <a name=\"import\"></a>\n",
    "\n",
    "[Retour table des matières](#up)\n",
    "\n",
    "Pour importer les textes de notre dataset, nous avons décidé de reprendre et adapter le code des auteurs du papier, afin d'être certains d'avoir exactement les mêmes données et ainsi pouvoir comparer nos résultats.\n",
    "\n",
    "\n",
    "Nous créons ainsi plusieurs listes.\n",
    "\n",
    "    - doc_name_list est la liste de tous les triplets (id, test/train, label) en strings.\n",
    "    - doc_train_list et doc_test_list contiennent, sous la même forme que la liste précédente, les triplets des test et train set respectivement.\n",
    "    - doc_content_list est la liste des textes.\n",
    "    - train_ids est la liste des id d'entrainement.\n",
    "    - train_ids_str est la même chose, mais sous forme de string avec les ids séparés par des \\n\n",
    "    - shuffle_doc_name_list correspond à doc_name_list, mais mélangé aléatoirement.\n",
    "    - shuffle_doc_words_list correspond à doc_content_list mais mélangé aléatoirement.\n",
    "    \n",
    "Cette partie du code (import des données) est fortement inspirée du [github des auteurs](https://github.com/yao8839836/text_gcn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51d3a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'R8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29776d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "f = open(os.path.join('text_gcn','data', 'R8.txt'), 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_name_list.append(line.strip())\n",
    "    temp = line.split(\"\\t\")\n",
    "    if temp[1].find('test') != -1:\n",
    "        doc_test_list.append(line.strip())\n",
    "    elif temp[1].find('train') != -1:\n",
    "        doc_train_list.append(line.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfaabff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content_list = []\n",
    "f = open('text_gcn/data/corpus/' + dataset + '.clean.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_content_list.append(line.strip())\n",
    "f.close()\n",
    "# print(doc_content_list)\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "#print(train_ids)\n",
    "random.shuffle(train_ids)\n",
    "\n",
    "# partial labeled data\n",
    "#train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
    "\n",
    "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "f = open('text_gcn/data/' + dataset + '.train.index', 'w')\n",
    "f.write(train_ids_str)\n",
    "f.close()\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "#print(test_ids)\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "f = open('text_gcn/data/' + dataset + '.test.index', 'w')\n",
    "f.write(test_ids_str)\n",
    "f.close()\n",
    "\n",
    "ids = train_ids + test_ids\n",
    "#print(ids)\n",
    "#print(len(ids))\n",
    "\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "\n",
    "f = open('text_gcn/data/' + dataset + '_shuffle.txt', 'w')\n",
    "f.write(shuffle_doc_name_str)\n",
    "f.close()\n",
    "\n",
    "f = open('text_gcn/data/corpus/' + dataset + '_shuffle.txt', 'w')\n",
    "f.write(shuffle_doc_words_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f3dac",
   "metadata": {},
   "source": [
    "# 2 Préparation des données <a name=\"init\"></a>\n",
    "\n",
    "[Retour table des matières](#up)\n",
    "\n",
    "Maintenant que nous avons importé les données sous les formes qui nous conviennent, nous devons les préparer.\n",
    "\n",
    "Nos données sont déjà partiellement préparées, puisque la ponctuation et les caractères spéciaux ont été retirés, et les textes mis en minuscules. \n",
    "\n",
    "En particulier, nous tokenizons les données, c'est à dire que nous séparons chaque document en liste de mots. Parmi ces mots, nous retirons les stopwords, c'est à dire les mots qui apparaissent très souvent et ne porte pas beaucoup de sens. C'est le cas notamment des pronoms, mais aussi de certains verbes comme être, avoir, etc. Il existe des listes de stopwords préfaites. En l'occurence nous utilisons une librairie qui elle même utilise une liste de stopwords issue de la librairie NLTK (spécialisée dans le NLP). Enfin, nous lemmatisons les mots. C'est à dire que nous les ramenons à leur racine. Par exemple, \"thinking\" et 'thoughts\" deviendront \"think\".\n",
    "\n",
    "Pour finir, nous retirons les mots qui apparaissent dans plus de 20% des documents. En effet, nous pouvons considérer qu'un mot qui apparait dans plus de 1 document sur 5 (parmi plusieurs milliers de textes) n'apporte pas beaucoup de sens. Il s'agit de stopwords également."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc1efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(data, no_below=1, no_above=0.2, max_words=100000):\n",
    "    \n",
    "    word_tok = Contract()\n",
    "    word_tok += Tokenize(stopwords=[\"en\",\"fr\"])\n",
    "    word_tok += Lemmatize()\n",
    "    word_tok += Phrase()\n",
    "    word_tok.verbose=False\n",
    "    #word_tok += Snowball(lang=\"french\") #stemming\n",
    "    \n",
    "    tokens=word_tok(data)\n",
    "    #print((tokens)) = liste des mots par doc\n",
    "    \n",
    "    dictionary = gensim.corpora.Dictionary(tokens)\n",
    "    #print(dictionary)\n",
    "    \n",
    "    #doesn't keep words that appear in less than no_below docs or more than no_above (ratio) docs\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=max_words)\n",
    "    #print(dictionary) \n",
    "    \n",
    "    return word_tok, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e41b841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tok, dictionary = prep(doc_content_list, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cb8158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(7088 unique tokens: ['also', 'annual', 'approved', 'approves', 'april']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8460e7",
   "metadata": {},
   "source": [
    "# 3 Calcul du TFIDF et du PMI<a name=\"dico\"></a>\n",
    "\n",
    "[Retour table des matières](#up)\n",
    "\n",
    "Nous calculons maintenant le score TFIDF, qui nous servira pour les poids des arêtes entres les liens et documents.\n",
    "\n",
    "Le score TFIDF traduit l'idée qu'un mot qui apparait beaucoup dans un document mais assez peu dans le corpus est probablement un mot important relativement au document. Un mot qui apparait très peu dans un texte est assez logiquement peu important (sauf s'il n'apparait presque jamais dans le reste du corpus !). Enfin un mot qui apparait souvent dans le document mais qui apparait également souvent dans le corpus n'est pas très important. Il s'agit également d'une forme de stopwords.\n",
    "\n",
    "Le score TFIDF est donc une mesure statistique basée sur la fréquence des mots.\n",
    "\n",
    "Il se calcule selon la formule qui suit :\n",
    "\n",
    "## INSERER FORMULE TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3f9be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(data, dictionary, word_tok, option=1):\n",
    "    \n",
    "    tokenized_data = word_tok(data)\n",
    "    \n",
    "    tokenized_data_idx = []\n",
    "    for i in range(len(data)):\n",
    "        tokenized_data_idx.append(dictionary.doc2idx(tokenized_data[i]))\n",
    "    \n",
    "    if option : #1\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in tokenized_data]\n",
    "        tfidf = TfidfModel(corpus) #fit\n",
    "        X = tfidf[corpus] #transform\n",
    "        TFIDF = matutils.corpus2csc(X,num_docs=len(X)).T\n",
    "    \n",
    "        vocab = list(dictionary.token2id.keys())\n",
    "        \n",
    "    else: #0\n",
    "        vectorizer = TfIdf(\n",
    "            min_df=1,\n",
    "            max_df=.5,\n",
    "            max_features=None,\n",
    "            verbose=False)\n",
    "\n",
    "        TFIDF = vectorizer(tokenized_data)\n",
    "        vocab = vectorizer.vectorizer.get_feature_names_out()\n",
    "        \n",
    "    #Version dense et datafrmame\n",
    "    #mat_dense = TFIDF.toarray()\n",
    "    #df_tfidf = pd.DataFrame(mat_dense, columns=vocab)    \n",
    "    \n",
    "    return TFIDF, tokenized_data, tokenized_data_idx, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a0f1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF, tokenized_data, tokenized_data_idx, vocab = compute_tfidf(doc_content_list, dictionary, word_tok, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4387a7",
   "metadata": {},
   "source": [
    "Nous calculons désormais le PMI (Pointwise Mutual Information). Cela nous servira pour les poids des arêtes entre deux mots. Le PMI mesure à quel point 2 mots sont susceptibles d'apparaitre ensemble. Il s'agit donc d'une mesure de la co-occurence. L'hypothèse faite est que deux mots qui apparaissent souvent ensemble sont susceptibles d'avoir un sens proche.\n",
    "\n",
    "Pour calculer le PMI, il faut faire glisser une fenêtre sur chacun des textes. La taille de cette fenêtre est un hyper-paramètre à choisir.\n",
    "\n",
    "En parcourant les documents, nous compterons : \n",
    "    - Le nombre total de fenêtres : #W\n",
    "    - Le nombre de fenêtres qui contiennent le mot i, pour chaque mot i : #W(i)\n",
    "    - Le nombre de fenêtres qui contiennent 2 mots i et j : #W(i,j)\n",
    "    \n",
    "Finalement, le PMI se calcule comme suit : \n",
    "\n",
    "# INSERER FORMULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d9d96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_word_pair(a, b):\n",
    "    if a > b:\n",
    "        return b, a\n",
    "    else:\n",
    "        return a, b\n",
    "        \n",
    "def update_word_and_word_pair_occurrence(q, W_i, W_ij):\n",
    "    '''\n",
    "    Traitement des mots de une fenêtre\n",
    "    \n",
    "    q : set d'indices de mots\n",
    "    '''\n",
    "    #On utilise un set pour avoir chaque mot une seule fois\n",
    "    #L'ordre n'est pas important\n",
    "    unique_q = list(set(q))\n",
    "    #On retire les mots qui n'apparaissent pas dans le dictionnaire\n",
    "    #Cad les mots dont la valeur est -1\n",
    "    if -1 in unique_q:\n",
    "        unique_q.remove(-1)\n",
    "    for w in unique_q:\n",
    "        #Update du nb de fentres ou apparait chaque mot\n",
    "        try:\n",
    "            W_i[w] += 1\n",
    "        except:\n",
    "            W_i[w] = 1 #Si la valeur n'existe pas encore\n",
    "    for i in range(len(unique_q)):\n",
    "        #update du nb de fenetres ou apparait chaque paire\n",
    "        for j in range(i+1, len(unique_q)):\n",
    "            #Pour chaque mot j après i \n",
    "            word1 = unique_q[i]\n",
    "            word2 = unique_q[j]\n",
    "            #On ne veut update que un coté de la matrice\n",
    "            word1, word2 = ordered_word_pair(word1, word2)\n",
    "            try:\n",
    "                W_ij[word1, word2] += 1\n",
    "            except:\n",
    "                W_ij[word1, word2] = 1\n",
    "    return W_i, W_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f88b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pmi(tokenized_data_idx, window_size=10, threshold=0.1, min_count=2, normalize=True):\n",
    "\n",
    "    W = 0 #compte des fenêtres\n",
    "\n",
    "    W_ij = dict()\n",
    "    W_i = dict()\n",
    "\n",
    "    #tokenized_data = liste des mots pour chaque doc\n",
    "    for words in tqdm(tokenized_data_idx):\n",
    "        #Pour chaque document, calcul de toutes les fréquences Wi Wij\n",
    "        #words = liste des indices des mots de 1 doc\n",
    "\n",
    "        #Initialisation de la fenêtre\n",
    "        q = []\n",
    "        for i in range(min(window_size, len(words))):\n",
    "            #On ajoute à la queue les indices des mots de la fenetre\n",
    "            #en partant du début du doc\n",
    "            q.append(words[i])\n",
    "        #Update du nb de fenetres\n",
    "        W+=1\n",
    "        #Update de W(i) et W(i,j)\n",
    "        W_i, W_ij = update_word_and_word_pair_occurrence(q, W_i, W_ij)\n",
    "\n",
    "        next_word_idx = window_size\n",
    "        #Tant qu'on a pas fini le doc, on décale la fenêtre\n",
    "        while next_word_idx<len(words):\n",
    "            #On décale la fenêtre\n",
    "            #En enlevant le premier mot\n",
    "            q.pop(0)\n",
    "            #Et en ajoutant le mot suivant (son indice)\n",
    "            q.append(words[next_word_idx])\n",
    "            next_word_idx+=1\n",
    "            W+=1\n",
    "            W_i, W_ij = update_word_and_word_pair_occurrence(q, W_i, W_ij)  \n",
    "    \n",
    "    PMI = dict()\n",
    "    #Pour chaque mot qui apparaissent ensemble\n",
    "    for (i,j), Wij in tqdm(W_ij.items()):\n",
    "        #On ne garde pas les couple qui apparaissent trop peu\n",
    "        if Wij < min_count:\n",
    "            continue\n",
    "        #word freq\n",
    "        Wi = W_i[i]\n",
    "        Wj = W_i[j]\n",
    "        pij = Wij/W\n",
    "        pi = Wi/W\n",
    "        pj = Wj/W\n",
    "\n",
    "        pmi=math.log(pij/(pi*pj))\n",
    "\n",
    "        #Normalisation du PMI\n",
    "        if normalize :\n",
    "            pmi = pmi/(-math.log(pij))\n",
    "        if pmi > threshold:\n",
    "            PMI[i,j]=pmi\n",
    "    return PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cb423e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 7674/7674 [00:05<00:00, 1359.56it/s]\n",
      "100%|██████████████████████████████| 987432/987432 [00:00<00:00, 3226225.84it/s]\n"
     ]
    }
   ],
   "source": [
    "PMI = compute_pmi(tokenized_data_idx, window_size=10, threshold=0.2, min_count=20, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a74dc0a",
   "metadata": {},
   "source": [
    "# test pmi importé de la librairie convectors (pour tester seulement et comparer ? a virer apres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2626e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convectors.linguistics import pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ee0888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pmi = pmi(tokenized_data,\n",
    "               normalize=True,\n",
    "               min_count=2,\n",
    "               window_size=10,\n",
    "               undirected=True,\n",
    "               minimum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c347bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29782\n",
      "23969\n"
     ]
    }
   ],
   "source": [
    "print(len(PMI))\n",
    "print(len(_pmi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8b0f5",
   "metadata": {},
   "source": [
    "# 4 Construction du graphe <a name=\"tfidf\"></a>\n",
    "\n",
    "[Retour table des matières](#up)\n",
    "\n",
    "Nous allons maintenant construire le graphe.\n",
    "\n",
    "Pour cela, nous commençons par créer un sommet par mot du vocabulaire, et une arête par couple de mots (qui ont un score PMI supérieur à un certain seuil) avec comme poids le score PMI.\n",
    "\n",
    "Ensuite nous créons une arête par couple (document, mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9c2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(PMI, TFIDF, vocab):\n",
    "    nodes = set()\n",
    "    edges = []\n",
    "    \n",
    "    #aretes entre les mots\n",
    "    #list_pmi = list(PMI.items())\n",
    "    for (u,v), w in PMI.items():\n",
    "        mot_u = vocab[u]\n",
    "        mot_v = vocab[v]\n",
    "        edges.append((mot_u,mot_v,w))\n",
    "        nodes.add(mot_u)\n",
    "        nodes.add(mot_v)\n",
    "    \n",
    "    l = len(edges)\n",
    "        \n",
    "    #aretes entre mots et doc\n",
    "    rows, cols = TFIDF.nonzero()\n",
    "    for r, c in zip(rows, cols):\n",
    "        #r = doc car ils sont sur les lignes\n",
    "        #c = mot car sur les colonnes\n",
    "        mot = vocab[c]\n",
    "        if mot not in nodes:\n",
    "            continue\n",
    "        triplet = (r, mot, TFIDF[r,c])\n",
    "        edges.append(triplet)  \n",
    "        \n",
    "    l1 = len(edges)\n",
    "    #construction du graphe\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(TFIDF.shape[0]))\n",
    "    G.add_weighted_edges_from(edges)\n",
    "\n",
    "    #Ajout des boucles\n",
    "    for node in G.nodes:\n",
    "        G.add_edge(node, node, weight=1)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d64ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data,\n",
    "                no_below=1,\n",
    "                no_above=0.2,\n",
    "                max_words=100000,\n",
    "                window_size=10,\n",
    "                pmi_threshold=0.2,\n",
    "                min_count=21,\n",
    "                normalize=True):\n",
    "    \n",
    "    word_tok, dictionary = prep(data, no_below, no_above)\n",
    "    \n",
    "    TFIDF, tokenized_data, tokenized_data_idx, vocab = compute_tfidf(data, dictionary, word_tok,0)\n",
    "    \n",
    "    n_docs, n_words = TFIDF.shape\n",
    "    \n",
    "    PMI = compute_pmi(tokenized_data_idx, window_size, pmi_threshold, min_count, normalize)\n",
    "\n",
    "    G = create_graph(PMI, TFIDF, vocab)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = build_graph(doc_content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83399030",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF.shape[0]+TFIDF.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f350c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "commu = louvain(G)\n",
    "print(len(set(commu.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.draw(G)\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test avec librairie textgraph\n",
    "\n",
    "G2 = TextGraph().fit_transform(doc_content_list)\n",
    "commu2 = louvain(G2)\n",
    "\n",
    "print(G2.number_of_nodes())\n",
    "print(G2.number_of_edges())\n",
    "\n",
    "print(len(set(commu2.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a2f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0cd59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15614968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7c3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba376ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
